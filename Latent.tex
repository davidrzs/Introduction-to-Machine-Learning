\section*{Missing data}
\subsection*{Mixture modeling}

Model each c. as probability distr. $P(x|\theta_j)$\\
$P(D|\theta) = \prod_{i=1}^n \sum_{j=1}^k w_j P(x_i|\theta_j)$\\
$L(w, \theta) = - \sum_{i=1}^n \operatorname{log}  \sum_{j=1}^k w_j P(x_i| \theta_j)$\\
\pseudosubsection{Gaussian-Mixture Bayes classifiers}\\
Estimate prior $P(y)$; Est. cond. distr. for each class:
$P(x|y) = \sum_{j=1}^{k_y} w_j^{(y)} \mathcal{N}(x; \mu_j^{(y)}, \Sigma_j^{(y)})$ where $\sum w_i = 1$ and \\
\pseudosubsection{EM Algorithm - Theory}\\
\textit{E-step} define the conditional expectation of the complete data log likelyhood wrt to the hidden variable $y$.\\
$Q(\theta|\theta^{(n)}) = \mathbb{E}_{X|y,\theta^{n}} [\log p_\theta(x,y)]$\\
\textit{M-step} find new $\theta$ maximizing expectation
$\theta^{(n+1)} = \operatorname{argmax}_{\theta} Q(\theta|\theta^{(n)})$\\
\pseudosubsection{Hard-EM algorithm}\\
Initialize parameters $\theta^{(0)}$\\
E-step: Predict most likely class for each point:
$z_i^{(t)} = \underset{z}{\operatorname{argmax}} ~ P(z|x_i, \theta^{(t-1)})\\
= {\operatorname{argmax}_z} ~ P(z|\theta^{(t-1)}) P(x_i|z,\theta^{(t-1)})$;\\
M-step: Compute the MLE: $\theta^{(t)} = \underset{\theta}{\operatorname{argmax}} \ P(D^{(t)}|\theta)$, i.e. $\mu_j^{(t)} = \frac{1}{n_j} \sum_{i: z_i = j} x_i$\\
\pseudosubsection{Soft-EM algorithm}\\
E-step: Calc p for each point and cls.: $\gamma_j^{(t)}(x_i)$\\
M-step: Fit clusters to weighted data points:\\
$w_j^{(t)} = \frac{1}{n} \sum_{i=1}^n \gamma_j^{(t)} (x_i)$; 
$\mu_j^{(t)} = \frac{\sum_{i=1}^n \gamma_j^{(t)} (x_i) x_i}{\sum_{i=1}^n \gamma_j^{(t)} (x_i)}\\ %\text{|learning with GMMs } ^t = ^*\\
\sigma_j^{(t)} = \frac{\sum_{i=1}^n \gamma_j^{(t)}(x_i) (x_i - \mu_j^{(t)})^T (x_i - \mu_j^{(t)})}{\sum_{i=1}^n \gamma_j^{(t)}(x_i)}$\\
\pseudosubsection{Soft-EM for semi-supervised learning}\\
labeled $y_i$: $\gamma_j^{(t)}(x_i) = [j = y_i]$,
unlabeled:\\ $\gamma_j^{(t)}(x_i) = P(Z=j|x_i, \mu^{(t-1)}, \Sigma^{(t-1)}, w^{(t-1)})$

%If enough space put this on summary.

\subsection*{E-step: Posterior probabilities}
$\gamma_j^t(x_i) = P(Z=j|x_i, \theta_t) = \frac{P(x_i|Z=j, \theta_t) P(Z=j|\theta_t)}{P(x_i;\theta_t)}$
