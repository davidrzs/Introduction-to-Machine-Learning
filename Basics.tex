
\section*{Basics}

\pseudosubsection{Fundamental Assumption}
Data is iid for unknown $P$: $(x_i, y_i) \sim P(X,Y)$\\
\pseudosubsection{Expected / Population Risk}
$R(f) = \mathbb{E}_{(x,y)\sim P(X,Y)}[l(f(x),y)] = \int P(x,y) \cdot (y-f(x))^2 \  dx dy$\\
\pseudosubsection{Population Minimizer}
$f^* = \operatorname{argmin}_{f\in\mathcal{F}} R(f)$\\
\pseudosubsection{Empirical Risk}
$\hat{R}_D (f) = \frac{1}{n} \sum_{i = 1}^{n} l (f(x_i),y_i) $\\
\pseudosubsection{Empirical Risk Minimization}
$\hat{f} = \operatorname{argmin}_{f\in\mathcal{F}} \hat{R}(f) $\\
\pseudosubsection{Bias/Variance/Noise}\\
Prediction error = $Bias^2 + Variance + Noise$\\
\pseudosubsection{Standardization}
Centered data with unit variance:
$\tilde{x}_{i} = \frac{x_{i}-\hat{\mu}}{\hat{\sigma}}$\\
$\hat{\mu} = \frac{1}{n}\sum_{i=1}^n x_{i}$, $\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n {(x_{i}-\hat{\mu})}^2$ \\
\pseudosubsection{Parametric vs. Nonparametric models}
\emph{Parametric}: have finite set of parameters. 
e.g. linear regression, linear perceptron\\
\emph{Nonparametric}: grow in complexity with the size of the data, more expressive.
e.g. k-NN
\\
\pseudosubsection{Gradient Descent}
1. Pick arbitrary $w_0 \in \mathbb{R}^d$\\
2. $w_{t+1} = w_t - \eta_t \nabla \hat{R}(w_t)$
\\
\pseudosubsection{Stochastic Gradient Descent (SGD)}
1. Pick arbitrary $w_0 \in \mathbb{R}^d$\\
2. $w_{t+1} = w_t - \eta_t \nabla_w l(w_t;x',y')$, with u.a.r.\\ data point $(x',y') \in D$
